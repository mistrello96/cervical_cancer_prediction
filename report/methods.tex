\section{METODI}
A seguito delle operazioni di \textit{preprocessing} del dataset, al fine di confrontare tra loro le varie tecniche di feature reduction, il \textit{workflow} si sviluppa in contemporanea su tre differenti livelli.
Ciascuno di essi gestisce privatamente una copia del dataset preprocessato.
Il primo livello fornisce in input agli algoritmi di apprendimento del modello il dataset senza che abbia subito alcuna operazione di feature reduction.
Il secondo fornisce come input all'algoritmo il risultato di un'operazione di feature extraction applicata al dataset mediante la tecnica della PCA. Questo approccio permette di trasformare un insieme di variabili correlate tra loro in un insieme ordinato di nuove feature non correlate tra loro; la trasformazione è definita in modo che la prima componente risulti essere quella con maggiore varianza, mentre tutte le successive mostrano il maggior valore di varianza possibile sotto il vincolo di ortogonalità con la precedente.
Al fine di stabilire la dimensionalità dello spazio di feature prodotto dalla PCA, è stato effettuato uno studio preliminare che consiste nella valutazione delle performance in 5-fold \textit{stratified cross validation} dei due algoritmi utilizzati al variare del numero di componenti nell'intervallo $[1, 33]$.
Infine, il terzo livello del \textit{workflow} fornisce come input agli algoritmi di apprendimento automatico il dataset a seguito di un'operazione di feature selection mediante l'utilizzo di un filtro basato sulla correlazione; al fine di individuare il threshold di selezione ottimale, è stato prodotto uno studio simile a quello realizzato per la PCA, testando valori i valori nel range [0,05, 0,95] e valutando le performance dei modelli.
La scelta di tale tecnica è stata dettata dal fatto che esso risulta appartenere alla classe dei filtri multivariati; ciò permette di individuare sia le feature irrilevanti sia quelle dipendenti tra loro, permettendo così di rimuovere la ridondanza dell'informazione. 
In aggiunta, i risultati evidenziati durante l'esplorazione del dataset (la presenza di feature altamente correlate tra loro) hanno suggerito la scelta di questa strategia, che risulta essere computazionalmente sostenibile dato il numero contenuto di sample presenti nel dataset.
Per ogni livello del \textit{workflow} sono stati utilizzati due differenti algoritmi di \textit{machine learning} per apprendere i rispettivi modelli; in particolare, sono stati impiegati alberi di decisione (a cui d'ora in poi ci si riferirà con DT in assenza di ambiguità) implementati da \textit{Weka} e random forest implementata nativamente in \texttt{Knime}.
Gli alberi di decisione sono una classe di tecniche di apprendimento automatico la cui struttura è composta da nodi interni che rappresentano test su una o più feature, mentre le foglie costituiscono i risultati della decisione.
L'algoritmo utilizzato per la creazione di questi alberi è \texttt{J48}, un'implementazione che estende la più classica \texttt{ID3} con il supporto delle feature con valori continui.
Al fine di stabilire quali siano i migliori test da effettuare all'interno dei nodi dell'albero viene valutata l'\textit{information gain}, una metrica in grado di evidenziare le divisioni in grado di generare sottogruppi nei quali l'entropia rispetto alla classe di appartenenza dei sample appartenenti al sottogruppo generato diminuisce rispetto all'entropia dell'insieme che li ha generati.
Le random forest sono un modello \textit{ensamble} costituito da un insieme predefinito di alberi di decisione, ognuno addestrato su un sottoinsieme differente di sample e feature.
Al momento della presentazione di un sample da classificare, ogni albero che compone la foresta propone la sua classificazione, che è poi valutata in modo pesato insieme a tutte le altre al fine di fornire una predizione complessiva.\\
Per confrontare le performance medie dei modelli derivanti dagli algoritmi appena presentati è stato sfruttato un processo di 5-fold \textit{stratified cross-validation}.
La scelta di utilizzare una tecnica di \textit{cross-validation} stratificata (SCV) è stata dettata dalla natura estremamente sbilanciata del problema, che ha portato altresì alla scelta di utilizzare un numero di fold contenuto, pari a $5$, in modo da mantenere in ogni fold un numero ragionevole di sample della classe minoritaria.
Data la scarsa rappresentazione della classe positiva presente nei vari train set, alla SCV è stata affiancata un'operazione di \textit{over-sampling} della classe minoritaria mediante la tecnica SMOTE. 
Questa tecnica permette di generare dei sample sintetici della classe minoritaria mediante la seguente procedura: per ogni campione della classe di minoranza vengono selezionati i \textit{k-nearest neighbors} del punto e si crea un sample tra il punto corrente e uno dei vicini scelto a caso. 
I valori delle feature del sample sintetico sono posizionate casualmente nello spazio che intercorre lungo la dimensione tra le due componenti dei due punti considerati; per far ciò si sfrutta l'estrazione di un numero casuale da una distribuzione uniforme nell'intervallo continuo $[0, 1]$.
L'operazione è stata effettuata esclusivamente sulla porzione di train di ogni iterazione della SCV, in modo da garantire che le performance misurate sul test set non fossero in alcuno modo influenzate.
L'utilizzo di una tecnica come SMOTE ha consentito di bilanciare le classi nei dati di training, permettendo così al modello di apprendere in modo più soddisfacente; è stato preferito l'\textit{over-sampling} all'\textit{under-sampling} in quanto quest'ultimo avrebbe ridotto drasticamente le dimensioni del train set, minando così le possibilità di apprendimento.
Si fa notare che il \textit{seed} per la generazione casuale dei fold è stato fissato in favore della ripetibilità dell'esperimento; tale \textit{seed} è inoltre condiviso tra tutti i generatori, in modo che vengano utilizzati i medesimi fold nei vari livelli del \textit{workflow}.
Al fine di stabilire se l'utilizzo da tale tecnica producesse un'alterazione delle performance nei modelli considerati, una verifica qualitativa è stata eseguita sui risultati degli algoritmi addestrati sull'intero dataset in presenza ed in assenza della tecnica di \textit{oversampling}.
Al termine delle operazioni di \textit{cross-validation}, i modelli prodotti dagli algoritmi sono stati confrontati tra loro basandosi sulla f1-\textit{measure} della classe minoritaria (\textit{i.e.}, presenza di cancro), calcolata come: \[2 \cdot \frac{precision \cdot recall}{precision+recall}.\]
L'utilizzo di una metrica come l'accuratezza o l'errore non s'addice al problema affrontato in quanto, considerato l'elevato grado di sbilanciamento del problema, un classificatore \textit{baseline} che predica esclusivamente l'assenza del tumore avrebbe ottenuto un'accuratezza molto elevata, pur non servendo allo scopo prefissato. 
Si è invece scelto di utilizzare una misura di performance come l'f1-\textit{measure} poiché in un problema di questo tipo è stato ritenuto rilevante considerare sia la \textit{precision}, ovvero la percentuale di veri positivi individuati rispetto al totale di predizioni positive fatte, sia la \textit{recall}, ovvero il numero di veri positivi individuati rispetto al numero totale che andava individuato. 
La motivazione alla base di questa decisione risulta essere triviale: si vorrebbero individuare correttamente il maggior numero possibile di soggetti affetti dalla patologia, mentre, al contempo, si vorrebbe evitare di diagnosticare erroneamente a molti pazienti di essere malati di cancro e costringerli così a dover affrontare ulteriori esami e operazioni invasive e costose; si è quindi optato per la medie armonica delle due metriche sopracitate (\textit{i.e.}, f1-\textit{measure}), in modo da bilanciare i falsi positivi con i falsi negativi.
Un incontro con esperti del dominio medico potrebbe portare ad una modifica della misura di performance utilizzata, prediligendo una delle due componenti dell'f1-\textit{measure} a discapito dell'altra; l'introduzione di una matrice dei costi che stimi il differente impatto economico e umano di falsi positivi e negativi potrebbe rivelarsi particolarmente utile in questo senso.\\
I confronti fra i vari modelli sono stati effettuati valutando le differenze tra la medie degli \textit{score} prodotti durante le varie iterazioni della \textit{cross-validation}; per stabilire se tali differenze fossero statisticamente significativa o meno sono stati utilizzati dei \textit{paired} t-test con un livello di confidenza pari a 0.95.
È stata utilizzata la versione \textit{paired} del test in quanto le performance dei vari modelli derivano i medesimi fold del processo di SCV.
Successivamente, sono stati confrontati fra loro i modelli ritenuti migliori per ogni livello del workflow (se sono risultati statisticamente differenti, altrimenti ne è stato selezionato uno a piacere), andando a verificare la presenza di un modello che risulti statisticamente migliore rispetto agli altri; anche questi test sono stati effettuati mediante un \textit{paired} t-test con un valore di confidenza fissato a 0.95.

Infine, sono state valutate le performance dei due algoritmi di apprendimento automatico proposti in assenza di alcune feature derivanti da esami clinici invasivi e costosi che, soprattutto in paesi di via di sviluppo, non risultano sempre disponibili o di facile accesso.
In particolare, dal dataset ottenuto a seguito della fase di \textit{preprocessing} sono state rimosse le feature relative ai 3 esami clinici effettuati: \textit{Hinselmann} (cioè la colposcopia), \textit{Schiller} (test eseguito durante la colposcopia) e \textit{Citology} (osservazione al microscopio di cellule ottenute mediante il Pap test o altre tecniche).
Il processo di addestramento dei modelli è stato eseguito nelle medesime condizioni sperimentali dei test precedenti, ovvero utilizzando un processo di 5-fold \textit{stratified cross-validation} con \textit{oversampling} sul train set.